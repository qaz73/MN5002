{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qaz73/MN5002/blob/main/Section%201/Section1Assignment_gorourke.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdMWHukOcnxL"
      },
      "source": [
        "# Section 1 Assignment - word vectors\n",
        "\n",
        "Student number : 24514772\n",
        "\n",
        "...This workbook follows the assignment templates in this order\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ea-_hbaqr0sQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiIM1DiJcnxM"
      },
      "source": [
        "The goals of the assignment include the following:\n",
        "* Checking your understanding of the basic operations and relationships between word vectors on a downloaded corpus using Gensim\n",
        "* Performing vector reasoning  on the corpus to determine answers to questions\n",
        "* Identifying and exploring deficiencies and biases in a corpus\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kd5sBgVZcnxM"
      },
      "source": [
        "## Download a pre-trained corpus\n",
        "Firstly ensure that you have Gensim installed and that it is the latest version. You will use Gensim to work with a corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsMQcNk0cnxN",
        "outputId": "97c940fb-2d39-48b8-f4b6-e5c174fdc0b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DpG4sOKcnxN"
      },
      "source": [
        "Next, import Gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CSy7Xw0RriA5"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import gensim.downloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrrYV3cHcnxN"
      },
      "source": [
        "Next set a download directory for your Gensim data. You will need to modify the path to the directory to your own needs. Some tips on this are given [here](https://radimrehurek.com/gensim/downloader.html#gensim.downloader.BASE_DIR)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1EckFY2GcnxO"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "#api.BASE_DIR = '/content/drive/MyDrive/MN5002'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WWO9Hi3cnxO"
      },
      "source": [
        "Now, download the Google News corpus cited by Tomas Mikolov et.al. in [Distributed Representations of Words and Phrases and their Compositionality](http://arxiv.org/pdf/1310.4546.pdf). It is a big file, so it can take a few minutes.\n",
        "\n",
        "**NOTE:** Especially in colab, when the file is done downloading you can see this error: FileNotFoundError: [Errno 2] No such file or directory: '/root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz'\n",
        "\n",
        "This is because the function is trying to also read the file (not just download it). In colab, it tries to read the file on a path that is different from the one we specified above. Double check the give location, you should still see that \"word2vec-google-news-300.gz\" was succesfully downloaded. If so, you can ignore the error and open the file as we did in \"gensim_word2vec_examples\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "b4n6p-gjcnxO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bed17e3-a5b1-43ad-b408-cec22906645d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__init__.py  __pycache__  word2vec-google-news-300.gz\n",
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/MN5002/word2vec-google-news-300\n",
        "google_news_vectors = api.load('word2vec-google-news-300')\n",
        "#from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "# - https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.load_word2vec_format.html\n",
        "# binary is a boolean indicating whether the data is in binary word2vec format\n",
        "# limit sets a maximum number of word-vectors to read from the file. The default, None, means read all.\n",
        "#google_news_vectors = KeyedVectors.load_word2vec_format(\"/content/drive/MyDrive/MN5002/word2vec-google-news-300/word2vec-google-news-300.gz\", binary=True, limit=200000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdbyc5DqriA6"
      },
      "source": [
        "Now it is time to explore the word vectors. You need to write code that uses [Gensim's word vector representations](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar) to answer the following questions and provide explanations of your answers using a Python print statement after each code step in your code, with careful consideration of how the meaning extracted from the corpus may be incorrect :\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. Knowing that the capital of Paris is France, use vector reasoning to find the capital of **Germany** and the capital of **Australia** and explain the answers"
      ],
      "metadata": {
        "id": "A0Z8fJDDFcTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar.html\n",
        "# Paris - France ~ capital - country\n",
        "# Paris - France ~ captial - Germany\n",
        "# Paris + Germany - France ~ Capital\n",
        "germany_capital = google_news_vectors.most_similar(positive=['Paris', 'Germany'], negative=['France'], topn=3)\n",
        "\n",
        "australia_capital = google_news_vectors.most_similar(positive=['Paris', 'Australia'], negative=['France'], topn=5)\n",
        "\n",
        "germany_capital, australia_capital"
      ],
      "metadata": {
        "id": "E9440bJ6IVjy",
        "outputId": "8d2a2459-2f75-40e4-a67d-566f22251e6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([('Berlin', 0.7644002437591553),\n",
              "  ('Frankfurt', 0.7329736351966858),\n",
              "  ('Dusseldorf', 0.7009457349777222)],\n",
              " [('Sydney', 0.7721031308174133),\n",
              "  ('Melbourne', 0.7192801833152771),\n",
              "  ('Canberra', 0.6764731407165527),\n",
              "  ('Brisbane', 0.6720366477966309),\n",
              "  ('Adelaide', 0.6661337614059448)])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2.  By considering the **USA** and by considering **Russia**, use vector reasoning to find the UK Prime Minister from the corpus and explain the answers you get."
      ],
      "metadata": {
        "id": "FXfGyzfMFlZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# v(President) - v(USA) ~ v(Prime Minister) - v(UK)\n",
        "# v(President) - v(USA) + v(UK) ~ v(Prime Minister)\n",
        "uk_prime_minister_via_usa = google_news_vectors.most_similar(positive=['President','UK'], negative=[ 'USA'], topn=3)\n",
        "uk_prime_minister_via_usa\n"
      ],
      "metadata": {
        "id": "Vbu3CXbeS1tZ",
        "outputId": "b51a1440-80e5-4db5-d0f0-64fe5151fce6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Chief_Executive', 0.4845598340034485),\n",
              " ('president', 0.4344619810581207),\n",
              " ('Tony_Blair', 0.43112385272979736)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3. What are the 5 most similar words to the word **BMW**? Explain your answer."
      ],
      "metadata": {
        "id": "wPZ1r6aNFo3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similar_bmw = google_news_vectors.most_similar('BMW', topn=5)\n",
        "similar_bmw"
      ],
      "metadata": {
        "id": "br09zbHiaVRr",
        "outputId": "6c571734-62ef-4b10-deed-50850e74e25b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Audi', 0.7932199835777283),\n",
              " ('Mercedes_Benz', 0.7683466672897339),\n",
              " ('Porsche', 0.7272197604179382),\n",
              " ('Mercedes', 0.7078384160995483),\n",
              " ('Volkswagen', 0.6959410905838013)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4. What are the 5 most similar words to the word **Tesla**? Explain your answer.\n"
      ],
      "metadata": {
        "id": "AQaW4ra3Fsyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similar_tesla = google_news_vectors.most_similar('Tesla', topn=5)\n",
        "similar_tesla"
      ],
      "metadata": {
        "id": "i5NQf18tajPp",
        "outputId": "5052d6fb-dff5-4e99-8efa-2bed488fe715",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Tesla_Motors', 0.720951497554779),\n",
              " ('Tesla_Roadster', 0.6531893014907837),\n",
              " ('afford_Nummi_Musk', 0.65052330493927),\n",
              " ('Telsa', 0.6308883428573608),\n",
              " ('electric_Tesla_Roadster', 0.6106851696968079)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5. Which of the words **battle** and **love** are closest to the word **fight**? Explain your answer."
      ],
      "metadata": {
        "id": "XXsE97F0Fu9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# higher score -> words closer\n",
        "sim_battle = google_news_vectors.similarity('fight', 'battle')\n",
        "sim_love = google_news_vectors.similarity('fight', 'love')\n",
        "print('similarity fight-battle : ', sim_battle)\n",
        "print('similarity fight-love : ', sim_love)"
      ],
      "metadata": {
        "id": "Jelsj_Q7bPth",
        "outputId": "f258dbd0-9f59-4a76-f58b-cd943c1fced2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "similarity fight-battle :  0.7021284\n",
            "similarity fight-love :  0.13506128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q6. Explore the corpus for gender bias in science. Start by considering that Albert Einstein is probably the most famous scientist. If you detect bias, elaborate on why it may be present, with specific examples, and what steps could be taken to address such bias."
      ],
      "metadata": {
        "id": "Hl3rArnBFw1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "einstein_bias = google_news_vectors.most_similar(positive=['Einstein', 'woman'], negative=['man'], topn=3)\n",
        "einstein_bias"
      ],
      "metadata": {
        "id": "eBLbVv8lt-tz",
        "outputId": "cda7f67e-17bf-4eca-fdf4-b7afa58a65f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Albert_Einstein', 0.5723960995674133),\n",
              " ('Michanowski', 0.49202975630760193),\n",
              " ('Poincar√©', 0.48435845971107483)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q7. How is a word vector represented as a Python data structure? Explain with an example from the corpus."
      ],
      "metadata": {
        "id": "HbEe1xbuFzTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Einstein_vector = google_news_vectors['Einstein']\n",
        "print (Einstein_vector)\n",
        "print (type(Einstein_vector))\n",
        "print (Einstein_vector.shape)"
      ],
      "metadata": {
        "id": "6gQnBycvcVkJ",
        "outputId": "40bf859d-5571-4026-84f3-1d4cf2697afa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2.14843750e-01  1.72851562e-01  2.91015625e-01  1.53320312e-01\n",
            " -1.92382812e-01  1.69677734e-02  3.86718750e-01 -1.92382812e-01\n",
            "  1.65039062e-01  6.59179688e-02  1.32812500e-01 -3.36914062e-02\n",
            " -1.86523438e-01  4.73632812e-02 -4.68750000e-02  1.37695312e-01\n",
            "  6.64062500e-02  1.94335938e-01  2.41210938e-01  2.92968750e-01\n",
            " -1.84570312e-01 -7.08007812e-02  1.94335938e-01 -5.88378906e-02\n",
            "  1.45263672e-02  7.56835938e-03 -2.57812500e-01 -1.72851562e-01\n",
            " -2.06054688e-01 -2.79296875e-01 -2.26562500e-01 -3.55468750e-01\n",
            "  6.62231445e-03  1.85546875e-02  5.34667969e-02  3.16406250e-01\n",
            "  4.83398438e-02  3.93066406e-02  1.04003906e-01  1.34765625e-01\n",
            " -2.35595703e-02  2.69531250e-01  1.35742188e-01  3.45703125e-01\n",
            "  1.73828125e-01  9.61914062e-02 -2.24609375e-01 -2.07031250e-01\n",
            " -1.79687500e-01 -7.47070312e-02 -5.19531250e-01  1.59912109e-02\n",
            " -4.49218750e-01 -2.67578125e-01  1.88476562e-01  2.63671875e-01\n",
            "  1.94335938e-01 -3.24218750e-01 -3.28125000e-01 -1.56250000e-01\n",
            "  2.75390625e-01  3.85742188e-02  2.27539062e-01 -1.83593750e-01\n",
            "  3.20312500e-01  5.54199219e-02 -2.34375000e-01  2.94921875e-01\n",
            "  8.64257812e-02  1.51367188e-02  1.21093750e-01  7.93457031e-03\n",
            "  6.34765625e-02 -1.99218750e-01 -1.48437500e-01  4.66308594e-02\n",
            "  1.11328125e-01  1.16210938e-01 -3.93066406e-02 -7.17773438e-02\n",
            "  9.57031250e-02 -1.46484375e-01  9.47265625e-02 -2.83203125e-01\n",
            "  8.74023438e-02  1.98242188e-01  2.81250000e-01  2.77343750e-01\n",
            " -3.04687500e-01 -8.30078125e-02 -1.29882812e-01 -1.73828125e-01\n",
            "  5.68847656e-02 -2.13867188e-01 -1.29882812e-01  3.76953125e-01\n",
            "  3.00781250e-01  1.82617188e-01  1.08398438e-01 -4.93164062e-02\n",
            "  1.44531250e-01 -3.20312500e-01  3.17382812e-02 -1.36718750e-01\n",
            "  1.65039062e-01  8.59375000e-02 -2.07031250e-01  1.04980469e-01\n",
            " -3.06640625e-01  2.92968750e-03 -4.44335938e-02  2.43164062e-01\n",
            "  7.56835938e-02  1.11328125e-01  3.75000000e-01 -1.01074219e-01\n",
            " -1.17675781e-01 -1.61132812e-01  3.26171875e-01 -1.72119141e-02\n",
            "  1.20239258e-02  1.40625000e-01 -1.69921875e-01 -1.31225586e-02\n",
            " -2.83203125e-01  7.76367188e-02  8.42285156e-03  2.45117188e-01\n",
            " -2.24609375e-01  2.98828125e-01  6.25000000e-02  3.76892090e-03\n",
            " -2.51953125e-01 -7.87353516e-03 -5.51757812e-02 -4.88281250e-02\n",
            "  1.08886719e-01  2.94921875e-01  5.78613281e-02  3.63281250e-01\n",
            "  2.16796875e-01 -1.25976562e-01 -3.15856934e-03  1.62353516e-02\n",
            "  6.68945312e-02  6.83593750e-02  1.06445312e-01 -2.85156250e-01\n",
            " -3.35937500e-01  5.66406250e-02  2.68554688e-02 -3.49609375e-01\n",
            " -9.52148438e-02  2.67578125e-01 -2.38281250e-01 -1.23046875e-01\n",
            "  3.10058594e-02  2.90527344e-02  9.27734375e-02  1.56250000e-01\n",
            "  8.88671875e-02 -3.22265625e-02  3.39843750e-01 -7.86132812e-02\n",
            "  1.12792969e-01 -1.34765625e-01  7.81250000e-02  4.51660156e-02\n",
            " -1.16210938e-01  1.78710938e-01  1.04003906e-01 -1.13281250e-01\n",
            " -8.72802734e-03  1.44531250e-01 -1.34765625e-01  2.18750000e-01\n",
            " -2.04101562e-01 -4.83398438e-02 -6.54296875e-02  3.44848633e-03\n",
            "  6.73828125e-02  6.22558594e-02 -4.23828125e-01 -2.41210938e-01\n",
            "  2.26562500e-01  1.14257812e-01  3.73535156e-02  1.07910156e-01\n",
            " -4.90722656e-02 -4.17480469e-02  1.22558594e-01  2.07031250e-01\n",
            "  3.14453125e-01  9.71679688e-02 -1.40625000e-01  1.39648438e-01\n",
            " -1.48437500e-01 -3.82812500e-01  6.83593750e-02  1.17187500e-01\n",
            " -1.37329102e-02 -1.89453125e-01 -8.15429688e-02  1.94335938e-01\n",
            " -1.17675781e-01 -1.52343750e-01 -2.38281250e-01  8.05664062e-02\n",
            " -3.26171875e-01  8.15429688e-02 -2.55859375e-01 -1.88476562e-01\n",
            "  1.29882812e-01 -1.50390625e-01 -3.88671875e-01  8.42285156e-03\n",
            "  2.18750000e-01  3.73046875e-01  2.12890625e-01  2.95410156e-02\n",
            "  3.41796875e-02  2.22656250e-01  3.04687500e-01 -7.27539062e-02\n",
            " -3.12500000e-01  4.07714844e-02  2.19726562e-01 -2.12402344e-02\n",
            "  1.92382812e-01 -9.91210938e-02  4.22363281e-02 -5.56640625e-02\n",
            " -1.14746094e-01 -2.69531250e-01  1.80664062e-01 -5.66406250e-02\n",
            "  1.14257812e-01 -1.04492188e-01 -2.18750000e-01 -1.98242188e-01\n",
            "  5.39550781e-02 -4.90722656e-02 -1.09375000e-01  6.44531250e-02\n",
            " -9.37500000e-02  1.07421875e-01 -9.27734375e-02  3.45703125e-01\n",
            " -1.47460938e-01 -8.54492188e-02 -2.63671875e-01 -8.54492188e-02\n",
            " -2.38037109e-03  2.00195312e-01  1.86523438e-01  4.63867188e-03\n",
            " -2.30468750e-01 -1.15722656e-01 -1.33789062e-01 -3.17096710e-05\n",
            "  2.61718750e-01  2.81982422e-02 -3.84765625e-01  1.40625000e-01\n",
            "  6.78710938e-02  9.13085938e-02 -2.52685547e-02 -2.63671875e-01\n",
            " -1.61132812e-01  4.49218750e-02  7.76367188e-02 -5.51757812e-02\n",
            " -1.02539062e-01  1.68457031e-02  3.24218750e-01 -1.18164062e-01\n",
            " -1.06811523e-02 -6.44531250e-02  2.05078125e-01  3.32641602e-03\n",
            " -6.93359375e-02  3.41796875e-01  1.62109375e-01  1.87500000e-01\n",
            " -4.58984375e-02  1.08398438e-01  2.03125000e-01 -2.02636719e-02\n",
            "  9.17968750e-02  1.10473633e-02 -1.85546875e-02  1.49414062e-01\n",
            " -3.55468750e-01 -8.74023438e-02  6.73828125e-02 -9.52148438e-02\n",
            " -1.19140625e-01  5.56640625e-02 -3.20312500e-01  1.52587891e-02]\n",
            "<class 'numpy.ndarray'>\n",
            "(300,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q8.  Which of these words is the odd one out? **California Texas Alaska India**"
      ],
      "metadata": {
        "id": "CktD50WNF1H0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "## https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.doesnt_match.html#gensim.models.Word2Vec.doesnt_match\n",
        "words = ['California','Texas','Alaska','India']\n",
        "result = google_news_vectors.doesnt_match(words)\n",
        "print (result)\n",
        "###########\n",
        "print('-'*40)\n",
        "word_pair_similarity = [(w1, w2, google_news_vectors.similarity(w1,w2)) for w1,w2 in combinations(words, 2)]\n",
        "word_pair_similarity_sorted = sorted(word_pair_similarity, key=lambda x: x[2], reverse=True)\n",
        "\n",
        "for ( w1, w2, score) in word_pair_similarity_sorted :\n",
        "    print(f\"{w1:15} {w2:15} {score}\")"
      ],
      "metadata": {
        "id": "r8k0ki7AhtCX",
        "outputId": "a8d3e2d4-8f4f-41fb-aa04-83fa0d81ea87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "India\n",
            "----------------------------------------\n",
            "California      Texas           0.46214014291763306\n",
            "California      Alaska          0.3903113007545471\n",
            "Texas           Alaska          0.338412344455719\n",
            "Texas           India           0.1825779527425766\n",
            "Alaska          India           0.18005137145519257\n",
            "California      India           0.17333202064037323\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b68f8234f4664cd92394ba08fd2bcacafca488f50355ff1b3e37ec2c2f0f7027"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}